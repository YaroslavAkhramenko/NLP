{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YaroslavAkhramenko/NLP/blob/main/Copy_of_%D0%92%D0%B0%D1%80%D0%B8%D0%B0%D0%BD%D1%82_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP5iCNKzqzUT"
      },
      "source": [
        "# Token classification на примере задачи NER (12 баллов)\n",
        "\n",
        "Это домашнее задание проходит в формате peer-review. Это означает, что его будут проверять ваши однокурсники. Поэтому пишите разборчивый код, добавляйте комментарии и пишите выводы после проделанной работы.\n",
        "\n",
        "Классификация токенов — задача, в которой для каждого отдельного токена или слова необходимо определить его тип, например, часть речи. В этом ноутбуке вам предстоит решить подвид задачи Token Classification, а именно NER или Named Entity Recognition. Вам необходимо для каждого слова определить, обозначает ли оно именованную сущность, например, имя человека, название места и тд."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CISIyjsLq3bK"
      },
      "source": [
        "Установим необходимые библиотеки: ```datasets```, ```transformers``` и ```seqeval```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0viZrfhqgyw",
        "outputId": "e0550475-ecb0-465c-b0ef-661f7bca491c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "sKuavd3rq7ig"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        "    )\n",
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WRgK1tYVq9_O",
        "outputId": "a9360774-ad38-433a-fce3-01a51d517eb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29tWdANmrBWI"
      },
      "source": [
        "## Подготовка данных\n",
        "\n",
        "Давайте поближе познакомимся с тем, как хранятся датасеты для NER. В этом задании вам предстоит работать с conll2003. Подробнее о нем можно узнать по этой [ссылке](https://huggingface.co/datasets/conll2003).\n",
        "\n",
        "В качестве предобученной модели воспользуемся DistilBERT. Это уменьшенная версия обычного BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "nBwhstlyrC1r"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYLMjfacrEwU"
      },
      "source": [
        "Загрузим данные с помощью функции load_dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "wM61EEV8rF7d"
      },
      "outputs": [],
      "source": [
        "datasets = load_dataset(\"conll2003\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keEwz30xrIoj"
      },
      "source": [
        "Наши данные состоят из следующих выборок:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iOZCyb9rJ-9",
        "outputId": "f3579b8a-4ada-4f58-a3e6-1ad138314d34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbopuTVhrNcV"
      },
      "source": [
        "В NER существует сразу несколько типов лэйблов для каждого токена. В случае с conll2003 существуют лэйблы следующих видов:\n",
        "\n",
        "* 'PER' для имен и фамилий\n",
        "* 'ORG' для названия организаций\n",
        "* 'LOC' для локаций\n",
        "* 'MISC' для смешанных сущностей\n",
        "* 'O' для обычных слов\n",
        "\n",
        "Также вначале лэйблов бывают буквы B и I. B означает начало сущности, I необходимо для следующего слова, означающего эту же сущность."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Puw88torrOuZ",
        "outputId": "8d64a00d-55c2-45f7-8d3f-f41d0e45f4e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "label_list = datasets[\"train\"].features[f\"ner_tags\"].feature.names\n",
        "label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH_CB1CvrQvm"
      },
      "source": [
        "Посмотрим на пример из датасета:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbR0agG0rSCZ",
        "outputId": "33c7d13d-7675-43e6-b343-eb7066e66b84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'])\n",
            "['Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.']\n",
            "[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "example = datasets[\"train\"][4]\n",
        "print(example.keys())\n",
        "print(example['tokens'])\n",
        "print(example['ner_tags'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwWmSCXhrUU6"
      },
      "source": [
        "Для каждого отдельного слова есть номер соответствующего лэйбла.\n",
        "\n",
        "Загрузим токенизатор:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "mg3rMBjOrXJM"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XHR2C9zrYs-"
      },
      "source": [
        "Вспомним, что модели семейства BERT используют subword токенизацию, то есть одно слово может получить несколько отдельных токенов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJil_6w-rZ38",
        "outputId": "4a8b684b-35da-47a3-e822-affa0c2d2c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'germany', \"'\", 's', 'representative', 'to', 'the', 'european', 'union', \"'\", 's', 'veterinary', 'committee', 'werner', 'z', '##wing', '##mann', 'said', 'on', 'wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']\n",
            "Всего слов: 31\n"
          ]
        }
      ],
      "source": [
        "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print(tokens)\n",
        "print(\"Всего слов:\", len(example[\"tokens\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww9tYQVFrcZA"
      },
      "source": [
        "Это означает, что нам необходимо конвертировать лэйблы таким образом, чтобы они соответствовали токенам.\n",
        "\n",
        "Для того, чтобы проверить к какому слову относится тот или иной токен удобно использовать следующую функцию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_tcD3TbrepE",
        "outputId": "8e15623d-533f-48fa-b301-01be87850b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_input.word_ids())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE0WhfM4rhWU"
      },
      "source": [
        "В исходном тексте было 31 слово, столько же индексов выдал и метод ```word_ids()```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYPGPa7frjJm"
      },
      "source": [
        "### Написание функции для преобразования лэйблов (3 балла)\n",
        "\n",
        "Ваша задача заключается в том, чтобы написать функцию ```tokenize_and_align_labels()```, которая должна делать токенизацию и преобразовывать лэйблы в формат, соответствующий токенам.\n",
        "\n",
        "То есть:\n",
        "* Если слово получило отдельный токен, то ему соответствует один лэйбл\n",
        "* Если слово получило несколько токенов, то ему должно соответствовать столько же лэйблов. Например, слово crisps получает токенизацию [15594, 2015], тогда в лэйблами для него будет [0, 0]\n",
        "* Если токен является служебным (имеет индекс None при вызове ```word_ids()```), то ему должен соответствовать лэйбл -100. Это специальный индекс, обозначающий те лэйблы, для которых не нужно считать лосс-функцию.\n",
        "\n",
        "Пример:\n",
        "\n",
        "Исходные слова: ```['Only', 'France',\n",
        " 'and',\n",
        " 'Britain',\n",
        " 'backed',\n",
        " 'Fischler',\n",
        " \"'s\",\n",
        " 'proposal',\n",
        " '.']```\n",
        "\n",
        " Исходные лэйблы: ```[0, 5, 0, 5, 0, 1, 0, 0, 0]```\n",
        "\n",
        " После токенизации: ```[101, 2069, 2605, 1998, 3725, 6153, 27424, 2818, 3917, 1005, 1055, 6378, 1012, 102]```\n",
        "\n",
        " Измененные лэйблы: ```[-100, 0, 5, 0, 5, 0, 1, 1, 1, 0, 0, 0, 0, -100]```\n",
        "\n",
        " Также дополнительные примеры можно посмотреть в следующих ячейках ноутбука, которые проверяют корректность реализации функции ```tokenize_and_align_labels()```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "LV0Ktqs0rmNt"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    print(examples)\n",
        "    # Токенизируем текст\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []  # В этот массив будем складывать id лэйблов токенов\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "\n",
        "        # Напишите код здесь. Соберите в список label_ids лэйблы, соответствующие токенам\n",
        "        label_ids = []\n",
        "        for j in word_ids:\n",
        "            if j is None:\n",
        "                label_ids.append(-100)\n",
        "            else:\n",
        "                label_ids.append(label[j])\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "bSJ-LOAsroMA"
      },
      "outputs": [],
      "source": [
        "test_examples = {\n",
        "    'id': ['0', '1', '2'],\n",
        "    'tokens': [\n",
        "        ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n",
        "        ['Peter', 'Blackburn'],\n",
        "        ['BRUSSELS', '1996-08-22']\n",
        "        ],\n",
        "    'ner_tags': [\n",
        "        [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
        "        [1, 2],\n",
        "        [5, 0]\n",
        "        ]\n",
        "    }\n",
        "\n",
        "test_outputs = {\n",
        "    'input_ids': [\n",
        "        [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102],\n",
        "        [101, 2848, 13934, 102],\n",
        "        [101, 9371, 2727, 1011, 5511, 1011, 2570, 102]\n",
        "        ],\n",
        "    'attention_mask': [\n",
        "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "        [1, 1, 1, 1],\n",
        "        [1, 1, 1, 1, 1, 1, 1, 1]\n",
        "        ],\n",
        "    'labels': [\n",
        "        [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100],\n",
        "        [-100, 1, 2, -100],\n",
        "        [-100, 5, 0, 0, 0, 0, 0, -100]\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt3WABYNrpv0",
        "outputId": "f13e70bc-cd62-41b6-aef8-868e87e9d141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': ['0', '1', '2'], 'tokens': [['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '1996-08-22']], 'ner_tags': [[3, 0, 7, 0, 0, 0, 7, 0, 0], [1, 2], [5, 0]]}\n"
          ]
        }
      ],
      "source": [
        "assert test_outputs == tokenize_and_align_labels(test_examples), \"Похоже tokenize_and_align_labels работает не так, как должна\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QexizBVzrslU"
      },
      "source": [
        "Применим функцию ко всем выборкам датасета с помощью метода ```map()```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "9To6te8yruGP"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66FvQqPLrwj2"
      },
      "source": [
        "## Тренировака модели\n",
        "\n",
        "\n",
        "Обучать модель будем с помощью ```Trainer``` из библиотеки ```transformers```.\n",
        "\n",
        "Загрузим претренированные веса:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8SSSLJ_rxz5",
        "outputId": "cd9fd55f-cba7-4517-8632-07007d6f5076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoGV5bkZr0Rp"
      },
      "source": [
        "### Определение аргументов для тренировки (1 балл)\n",
        "\n",
        "Загляните в [документацию](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) и заполните необходимые аргументы для тренировки. Помните, что для файнтюнинга больших моделей следует выбирать небольшой learning rate (обычно меньше 1е-5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPyHxzIelClG",
        "outputId": "19ec5040-4a3a-408d-858b-fce78e105ab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: accelerate\n",
            "Version: 0.21.0\n",
            "Summary: Accelerate\n",
            "Home-page: https://github.com/huggingface/accelerate\n",
            "Author: The HuggingFace team\n",
            "Author-email: sylvain@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: numpy, packaging, psutil, pyyaml, torch\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "rS6w-Tqdr17S"
      },
      "outputs": [],
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='./res',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayyZSHrjr3ZR"
      },
      "source": [
        "Создадим вспомогательные объекты: ```DataCollatorForTokenClassification``` и ```metric```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "3i1EzFxgr48L"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load_metric(\"seqeval\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq75CLiPr7Tb"
      },
      "source": [
        "### Расчет метрики (1 балл)\n",
        "\n",
        "Опишем функцию ```compute_metric```, которая будет учитывать только нужные токены."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x55dL77Tr8e7",
        "outputId": "66003eb3-be65-4843-d041-7e3571aa1716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(label_list)\n",
        "\n",
        "def compute_metrics(predictions, labels):\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Удалим из подсчета метрик служебные токены\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # По аналогии с фильтрацией true_predictions опишите фильтрацию для true_labels\n",
        "    true_labels =  [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]# магия здесь\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOUdZvrhr-MM"
      },
      "source": [
        "### Использование ```Trainer``` для обучения (2 балла)\n",
        "\n",
        "Далее создайте объект класса ```Trainer``` с необходимыми аргументами и обучите модель.\n",
        "\n",
        "Подробнее о том, как использовать ```Trainer```, можно почитать [здесь](https://huggingface.co/docs/transformers/main_classes/trainer) или же посмотреть семинарское занятия из этого модуля.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "R1BzxvDbr_Zd",
        "outputId": "be6b7b79-ba5c-4fb9-ec33-3f3490ef639a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3512' max='3512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3512/3512 05:52, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.348800</td>\n",
              "      <td>0.124168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.072100</td>\n",
              "      <td>0.112804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.049900</td>\n",
              "      <td>0.119309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.039000</td>\n",
              "      <td>0.122487</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3512, training_loss=0.10191591927497425, metrics={'train_runtime': 352.3106, 'train_samples_per_second': 159.416, 'train_steps_per_second': 9.968, 'total_flos': 682592149798524.0, 'train_loss': 0.10191591927497425, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Создайте объект класса Trainer и обучите модель\n",
        "#print(datasets['train'])\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWzf7Ib1sAlM"
      },
      "source": [
        "### Получение необходимой метрики (3 балла)\n",
        "\n",
        " Хорошее качество для этой задачи ~0.92 по F1 мере или выше. Попробуйте добиться этого значения, используя различные гиперпараметры в ```TrainingArguments```. Напишите вывод о проделанной работе."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "sxZ0IZ38sBu4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "cb21e4ee-cd23-4939-d3fb-9126982ca5c8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'precision': 0.9152355790400705,\n",
              " 'recall': 0.9300816646157288,\n",
              " 'f1': 0.922598901403762,\n",
              " 'accuracy': 0.9821595945795669}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# Код\n",
        "compute_metrics(trainer.predict(tokenized_datasets['validation']).predictions, tokenized_datasets['validation']['labels'])\n",
        "#Я менял\n",
        "#1) batch_size - оптимальный 16,\n",
        "#2) learning_rate - при 1е-6 получилось 0.89. При 4е-5 получилось 0.93758\n",
        "#Исходя из затраченого времени, количество эпох более 4х не ставилось"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l63ePuqWsDfy"
      },
      "source": [
        "### Дополнительный эксперимент (2 балла)\n",
        "\n",
        "А теперь попробуйте решить ту же задачу, но с другой претренированной моделью из семейства BERT, например, ```roberta-base``` или ```distillroberta-base``` и получить качество выше 0.94 по F1 на валидационном датасете. Список доступных моделей можно посмотреть [здесь](https://huggingface.co/models). Вы на практике убедитесь, насколько различные претренированные модели могут улучшать конечное качество на downstream задачах.\n",
        "\n",
        "Для выполнения этого пункта можно всего лишь скопировать некоторые ячейки кода выше и поменять переменную ```model_checkpoint``` на название другой модели."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXargs = TrainingArguments(\n",
        "    output_dir='./res',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        ")"
      ],
      "metadata": {
        "id": "cLUErEmiKpky"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "hdjcyxrysE7F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "65fdf0a5-4e98-41c6-c188-1df40edc9072"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-ac47614bb18a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Проведите эксперимент здесь\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m EXtrainer = Trainer(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dslim/bert-base-NER'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEXargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;31m# Quantized models doesn't support `.to` operation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_model_on_device\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is_quantized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
          ]
        }
      ],
      "source": [
        "# Проведите эксперимент здесь\n",
        "\n",
        "EXtrainer = Trainer(\n",
        "    model='dslim/bert-base-NER',\n",
        "    args=EXargs,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = data_collator\n",
        ")\n",
        "\n",
        "EXtrainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_metrics(EXtrainer.predict(tokenized_datasets['validation']).predictions, tokenized_datasets['validation']['labels'])"
      ],
      "metadata": {
        "id": "lpbPjL77EghO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}