{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YaroslavAkhramenko/NLP/blob/main/%D0%92%D0%B0%D1%80%D0%B8%D0%B0%D0%BD%D1%82_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token classification на примере задачи NER (12 баллов)\n",
        "\n",
        "Это домашнее задание проходит в формате peer-review. Это означает, что его будут проверять ваши однокурсники. Поэтому пишите разборчивый код, добавляйте комментарии и пишите выводы после проделанной работы.\n",
        "\n",
        "Классификация токенов — задача, в которой для каждого отдельного токена или слова необходимо определить его тип, например, часть речи. В этом ноутбуке вам предстоит решить подвид задачи Token Classification, а именно NER или Named Entity Recognition. Вам необходимо для каждого слова определить, обозначает ли оно именованную сущность, например, имя человека, название места и тд."
      ],
      "metadata": {
        "id": "OP5iCNKzqzUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установим необходимые библиотеки: ```datasets```, ```transformers``` и ```seqeval```."
      ],
      "metadata": {
        "id": "CISIyjsLq3bK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "D0viZrfhqgyw",
        "outputId": "f36bfc9d-cf9a-4d17-ba97-0a898644a0cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        "    )\n",
        "from datasets import load_dataset, load_metric"
      ],
      "metadata": {
        "id": "sKuavd3rq7ig"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "WRgK1tYVq9_O",
        "outputId": "16a83f23-bfba-4afb-d258-bc779292c474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка данных\n",
        "\n",
        "Давайте поближе познакомимся с тем, как хранятся датасеты для NER. В этом задании вам предстоит работать с conll2003. Подробнее о нем можно узнать по этой [ссылке](https://huggingface.co/datasets/conll2003).\n",
        "\n",
        "В качестве предобученной модели воспользуемся DistilBERT. Это уменьшенная версия обычного BERT."
      ],
      "metadata": {
        "id": "29tWdANmrBWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "nBwhstlyrC1r"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим данные с помощью функции load_dataset."
      ],
      "metadata": {
        "id": "AYLMjfacrEwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "wM61EEV8rF7d"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наши данные состоят из следующих выборок:"
      ],
      "metadata": {
        "id": "keEwz30xrIoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets"
      ],
      "metadata": {
        "id": "3iOZCyb9rJ-9",
        "outputId": "fc1c9e50-faee-49ac-8599-1b9b592a4557",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В NER существует сразу несколько типов лэйблов для каждого токена. В случае с conll2003 существуют лэйблы следующих видов:\n",
        "\n",
        "* 'PER' для имен и фамилий\n",
        "* 'ORG' для названия организаций\n",
        "* 'LOC' для локаций\n",
        "* 'MISC' для смешанных сущностей\n",
        "* 'O' для обычных слов\n",
        "\n",
        "Также вначале лэйблов бывают буквы B и I. B означает начало сущности, I необходимо для следующего слова, означающего эту же сущность."
      ],
      "metadata": {
        "id": "tbopuTVhrNcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = datasets[\"train\"].features[f\"ner_tags\"].feature.names\n",
        "label_list"
      ],
      "metadata": {
        "id": "Puw88torrOuZ",
        "outputId": "d318743b-6420-40b7-f6a0-71d86ff45a27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на пример из датасета:"
      ],
      "metadata": {
        "id": "GH_CB1CvrQvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = datasets[\"train\"][4]\n",
        "print(example.keys())\n",
        "print(example['tokens'])\n",
        "print(example['ner_tags'])"
      ],
      "metadata": {
        "id": "mbR0agG0rSCZ",
        "outputId": "3fcd40bc-4536-4c5a-bc54-3e4665468d6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'])\n",
            "['Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.']\n",
            "[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для каждого отдельного слова есть номер соответствующего лэйбла.\n",
        "\n",
        "Загрузим токенизатор:"
      ],
      "metadata": {
        "id": "zwWmSCXhrUU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "mg3rMBjOrXJM"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вспомним, что модели семейства BERT используют subword токенизацию, то есть одно слово может получить несколько отдельных токенов."
      ],
      "metadata": {
        "id": "3XHR2C9zrYs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print(tokens)\n",
        "print(\"Всего слов:\", len(example[\"tokens\"]))"
      ],
      "metadata": {
        "id": "cJil_6w-rZ38",
        "outputId": "3f346c50-7363-4482-a2ca-dc9ee75d10ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'germany', \"'\", 's', 'representative', 'to', 'the', 'european', 'union', \"'\", 's', 'veterinary', 'committee', 'werner', 'z', '##wing', '##mann', 'said', 'on', 'wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']\n",
            "Всего слов: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это означает, что нам необходимо конвертировать лэйблы таким образом, чтобы они соответствовали токенам.\n",
        "\n",
        "Для того, чтобы проверить к какому слову относится тот или иной токен удобно использовать следующую функцию:"
      ],
      "metadata": {
        "id": "Ww9tYQVFrcZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_input.word_ids())"
      ],
      "metadata": {
        "id": "q_tcD3TbrepE",
        "outputId": "a124c2d8-27ea-4b6c-e433-1e9ff40d9745",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В исходном тексте было 31 слово, столько же индексов выдал и метод ```word_ids()```"
      ],
      "metadata": {
        "id": "qE0WhfM4rhWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Написание функции для преобразования лэйблов (3 балла)\n",
        "\n",
        "Ваша задача заключается в том, чтобы написать функцию ```tokenize_and_align_labels()```, которая должна делать токенизацию и преобразовывать лэйблы в формат, соответствующий токенам.\n",
        "\n",
        "То есть:\n",
        "* Если слово получило отдельный токен, то ему соответствует один лэйбл\n",
        "* Если слово получило несколько токенов, то ему должно соответствовать столько же лэйблов. Например, слово crisps получает токенизацию [15594, 2015], тогда в лэйблами для него будет [0, 0]\n",
        "* Если токен является служебным (имеет индекс None при вызове ```word_ids()```), то ему должен соответствовать лэйбл -100. Это специальный индекс, обозначающий те лэйблы, для которых не нужно считать лосс-функцию.\n",
        "\n",
        "Пример:\n",
        "\n",
        "Исходные слова: ```['Only', 'France',\n",
        " 'and',\n",
        " 'Britain',\n",
        " 'backed',\n",
        " 'Fischler',\n",
        " \"'s\",\n",
        " 'proposal',\n",
        " '.']```\n",
        "\n",
        " Исходные лэйблы: ```[0, 5, 0, 5, 0, 1, 0, 0, 0]```\n",
        "\n",
        " После токенизации: ```[101, 2069, 2605, 1998, 3725, 6153, 27424, 2818, 3917, 1005, 1055, 6378, 1012, 102]```\n",
        "\n",
        " Измененные лэйблы: ```[-100, 0, 5, 0, 5, 0, 1, 1, 1, 0, 0, 0, 0, -100]```\n",
        "\n",
        " Также дополнительные примеры можно посмотреть в следующих ячейках ноутбука, которые проверяют корректность реализации функции ```tokenize_and_align_labels()```"
      ],
      "metadata": {
        "id": "aYPGPa7frjJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_all_tokens = True\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    # Токенизируем текст\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []  # В этот массив будем складывать id лэйблов токенов\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "\n",
        "        # Напишите код здесь. Соберите в список label_ids лэйблы, соответствующие токенам\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n"
      ],
      "metadata": {
        "id": "LV0Ktqs0rmNt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_examples = {\n",
        "    'id': ['0', '1', '2'],\n",
        "    'tokens': [\n",
        "        ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n",
        "        ['Peter', 'Blackburn'],\n",
        "        ['BRUSSELS', '1996-08-22']\n",
        "        ],\n",
        "    'ner_tags': [\n",
        "        [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
        "        [1, 2],\n",
        "        [5, 0]\n",
        "        ]\n",
        "    }\n",
        "\n",
        "test_outputs = {\n",
        "    'input_ids': [\n",
        "        [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102],\n",
        "        [101, 2848, 13934, 102],\n",
        "        [101, 9371, 2727, 1011, 5511, 1011, 2570, 102]\n",
        "        ],\n",
        "    'attention_mask': [\n",
        "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "        [1, 1, 1, 1],\n",
        "        [1, 1, 1, 1, 1, 1, 1, 1]\n",
        "        ],\n",
        "    'labels': [\n",
        "        [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100],\n",
        "        [-100, 1, 2, -100],\n",
        "        [-100, 5, 0, 0, 0, 0, 0, -100]\n",
        "        ]\n",
        "    }"
      ],
      "metadata": {
        "id": "bSJ-LOAsroMA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_outputs == tokenize_and_align_labels(test_examples), \"Похоже tokenize_and_align_labels работает не так, как должна\""
      ],
      "metadata": {
        "id": "zt3WABYNrpv0",
        "outputId": "baa6c3d2-0df7-4ad0-8f4a-88b2ff9dc9ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-caab3a65d303>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtest_outputs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtokenize_and_align_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Похоже tokenize_and_align_labels работает не так, как должна\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-18a79cdadfaa>\u001b[0m in \u001b[0;36mtokenize_and_align_labels\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Напишите код здесь. Соберите в список label_ids лэйблы, соответствующие токенам\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtokenized_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'label_ids' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Применим функцию ко всем выборкам датасета с помощью метода ```map()```"
      ],
      "metadata": {
        "id": "QexizBVzrslU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "9To6te8yruGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Тренировака модели\n",
        "\n",
        "\n",
        "Обучать модель будем с помощью ```Trainer``` из библиотеки ```transformers```.\n",
        "\n",
        "Загрузим претренированные веса:"
      ],
      "metadata": {
        "id": "66FvQqPLrwj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
      ],
      "metadata": {
        "id": "E8SSSLJ_rxz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Определение аргументов для тренировки (1 балл)\n",
        "\n",
        "Загляните в [документацию](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) и заполните необходимые аргументы для тренировки. Помните, что для файнтюнинга больших моделей следует выбирать небольшой learning rate (обычно меньше 1е-5)."
      ],
      "metadata": {
        "id": "UoGV5bkZr0Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "args = TrainingArguments(\n",
        "    # Опишите здесь необходимые аргументы\n",
        ")"
      ],
      "metadata": {
        "id": "rS6w-Tqdr17S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим вспомогательные объекты: ```DataCollatorForTokenClassification``` и ```metric```"
      ],
      "metadata": {
        "id": "ayyZSHrjr3ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load_metric(\"seqeval\")"
      ],
      "metadata": {
        "id": "3i1EzFxgr48L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Расчет метрики (1 балл)\n",
        "\n",
        "Опишем функцию ```compute_metric```, которая будет учитывать только нужные токены."
      ],
      "metadata": {
        "id": "Hq75CLiPr7Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(predictions, labels):\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Удалим из подсчета метрик служебные токены\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # По аналогии с фильтрацией true_predictions опишите фильтрацию для true_labels\n",
        "    true_labels =  # магия здесь\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "x55dL77Tr8e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Использование ```Trainer``` для обучения (2 балла)\n",
        "\n",
        "Далее создайте объект класса ```Trainer``` с необходимыми аргументами и обучите модель.\n",
        "\n",
        "Подробнее о том, как использовать ```Trainer```, можно почитать [здесь](https://huggingface.co/docs/transformers/main_classes/trainer) или же посмотреть семинарское занятия из этого модуля.  "
      ],
      "metadata": {
        "id": "oOUdZvrhr-MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создайте объект класса Trainer и обучите модель"
      ],
      "metadata": {
        "id": "R1BzxvDbr_Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Получение необходимой метрики (3 балла)\n",
        "\n",
        " Хорошее качество для этой задачи ~0.92 по F1 мере или выше. Попробуйте добиться этого значения, используя различные гиперпараметры в ```TrainingArguments```. Напишите вывод о проделанной работе."
      ],
      "metadata": {
        "id": "wWzf7Ib1sAlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Код"
      ],
      "metadata": {
        "id": "sxZ0IZ38sBu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Дополнительный эксперимент (2 балла)\n",
        "\n",
        "А теперь попробуйте решить ту же задачу, но с другой претренированной моделью из семейства BERT, например, ```roberta-base``` или ```distillroberta-base``` и получить качество выше 0.94 по F1 на валидационном датасете. Список доступных моделей можно посмотреть [здесь](https://huggingface.co/models). Вы на практике убедитесь, насколько различные претренированные модели могут улучшать конечное качество на downstream задачах.\n",
        "\n",
        "Для выполнения этого пункта можно всего лишь скопировать некоторые ячейки кода выше и поменять переменную ```model_checkpoint``` на название другой модели."
      ],
      "metadata": {
        "id": "l63ePuqWsDfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Проведите эксперимент здесь"
      ],
      "metadata": {
        "id": "hdjcyxrysE7F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}